{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "veN0lOz4QqU5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_np_array(file_name):\n",
    "    X_array = np.load('data/X_' + file_name + '_array.npy')\n",
    "    y_array = np.load('data/y_' + file_name + '_array.npy')\n",
    "    return X_array, y_array\n",
    "\n",
    "# Convert numerical data to strings\n",
    "def convert_to_string(data):\n",
    "    return [\" \".join(map(str, seq.flatten())) for seq in data]\n",
    "\n",
    "# Define the compute_metrics function to calculate accuracy\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(labels, pred)\n",
    "    return {\"eval_accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Create Dataset class\n",
    "class FallDetectionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5824, 40, 3)\n",
      "y_train shape: (5824,)\n",
      "X_test shape: (2912, 40, 3)\n",
      "y_test shape: (2912,)\n",
      "\n",
      "Example X_train_text: ['1.1855469 0.5124512 0.19116211 1.0292969 0.5078125 0.35766602 0.9499512 0.7338867 0.80615234 1.1665039 0.8579102 1.0546875 1.2587892 0.7905274 1.1374512 1.0974121 0.3955078 1.1479492 0.28881836 0.20166017 1.142334 0.001220703 -0.5805664 1.279297 -0.56884766 -1.6184083 1.4101563 -0.5275879 -1.7941896 0.98657227 -0.72875977 -1.9460449 0.7573242 -0.7702637 -1.5083008 0.4157715 -0.5095215 -1.6040039 0.4375 -0.39453128 -1.4926758 0.23291016 -0.22192383 -1.0761719 0.26586914 -0.20288086 -1.3786621 0.047607422 -0.27270508 -0.9104004 -0.087890625 -0.2775879 -1.0041504 -0.07666016 -0.2932129 -0.8710938 0.011962892 -0.34057617 -0.7502442 0.13989258 -0.34057617 -0.7502442 0.13989258 -0.41137698 -0.56762695 0.23486328 -0.30639648 -0.35546875 0.42626953 -0.16040039 -0.24780275 0.7351074 -0.24316406 -0.33593753 0.7641602 -0.16113281 -0.072509766 0.80322266 -0.060058594 -0.34423828 0.7402344 -0.07128906 -0.3820801 0.8305664 -0.07763672 -0.49731445 0.9152832 0.061523438 -0.36230472 0.9487305 0.17260742 -0.23266602 1.0319824 0.15063477 -0.2800293 1.0900879 0.09814453 -0.31225586 1.0297852 0.18408205 -0.37670898 1.0227051 0.11010742 -0.30029297 1.0270996 0.17309572 -0.31079102 1.0412598 0.13916016 -0.33251953 0.8776856 0.17944336 -0.21972656 0.88208014 0.46069336 -0.3110352 0.95336914 0.46337894 -0.3886719 0.83691406', '0.5812988 0.39379886 0.29052734 0.49951172 0.40966797 0.28588867 0.42700195 0.4272461 0.2788086 0.5415039 0.5402832 0.4482422 0.5415039 0.5402832 0.4482422 0.6262207 0.6274414 0.5625 0.6228028 0.61938477 0.6911621 0.32421878 0.5109863 0.7680664 0.29418945 0.4355469 0.7529297 0.06665039 0.28857422 0.5546875 0.28344727 -0.45410156 0.26416016 0.28344727 -0.45410156 0.26416016 0.5678711 -0.59765625 -0.100830086 0.7155762 -1.2036133 -0.6066895 1.5107423 -0.92895514 -2.145996 5.694824 7.9990234 -0.3010254 2.7395022 2.0078125 0.33203125 -1.5163574 0.7285156 -0.7470703 0.33203125 1.3190918 -0.072509766 -0.4619141 1.2255859 -0.37988284 -0.71484375 0.5300293 0.028320314 -0.46435547 0.87158203 -0.72924805 -0.15307617 0.64184576 -0.96313477 -0.31323245 0.53149414 -0.86987305 -0.21362305 0.6020508 -0.8942871 -0.23046875 0.5148926 -0.48828128 -0.25634766 0.47656253 -0.88647467 -0.6086426 0.42382815 -0.7602539 -0.39282227 0.33544922 -0.99438477 -0.57836914 0.22656251 -0.842041 -0.484375 0.18334962 -0.83618164 -0.484375 0.18334962 -0.83618164 -0.48657227 0.12988281 -0.8071289 -0.486084 0.12841797 -0.78247076 -0.472168 0.20092775 -0.888916 -0.51342773 0.23242189 -0.8701172 -0.4956055 0.29296875 -0.8876953 -0.5019531 0.30615234 -0.88012695 -0.5197754 0.33886722 -0.8078613 -0.5024414 0.40429688 -0.8535157']\n",
      "Example y_train: [0 1]\n"
     ]
    }
   ],
   "source": [
    "X_train_fall, y_train_fall = load_np_array(\"train_fall\")\n",
    "X_train_notfall, y_train_notfall = load_np_array(\"train_notfall\")\n",
    "X_test_fall, y_test_fall = load_np_array(\"test_fall\")\n",
    "X_test_notfall, y_test_notfall = load_np_array(\"test_notfall\")\n",
    "\n",
    "# Combine data\n",
    "X_train = np.concatenate((X_train_fall, X_train_notfall), axis=0)\n",
    "y_train = np.concatenate((y_train_fall, y_train_notfall), axis=0)\n",
    "X_test = np.concatenate((X_test_fall, X_test_notfall), axis=0)\n",
    "y_test = np.concatenate((y_test_fall, y_test_notfall), axis=0)\n",
    "\n",
    "# Shuffle data\n",
    "train_indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(train_indices)\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "test_indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(test_indices)\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]\n",
    "\n",
    "X_train_text = convert_to_string(X_train)\n",
    "X_test_text = convert_to_string(X_test)\n",
    "\n",
    "# Debugging: Print shapes and example data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"\\nExample X_train_text:\", X_train_text[:2])\n",
    "print(\"Example y_train:\", y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxDxZ37YQs3d",
    "outputId": "2ae5f1b3-abeb-4e2e-a2ba-2e451fdd1c36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and Model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "max_length = 40\n",
    "\n",
    "# Split train data into train and validation sets\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42  # Adjust random_state for reproducibility\n",
    ")\n",
    "\n",
    "# Convert numerical data to strings\n",
    "X_train_text_split = convert_to_string(X_train_split)\n",
    "X_val_text = convert_to_string(X_val)\n",
    "\n",
    "# Create Dataset class\n",
    "train_dataset_split = FallDetectionDataset(X_train_text_split, y_train_split, tokenizer, max_length)\n",
    "val_dataset = FallDetectionDataset(X_val_text, y_val, tokenizer, max_length)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader_split = DataLoader(train_dataset_split, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = FallDetectionDataset(X_train_text, y_train, tokenizer, max_length)\n",
    "test_dataset = FallDetectionDataset(X_test_text, y_test, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "m56dtReVQx6o",
    "outputId": "933d9c15-10eb-4ad5-9cbd-a57f4b10c3f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 35/250 3:34:07 < 23:15:06, 0.00 it/s, Epoch 7/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.685325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.665158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.622544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.604725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.596074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.601093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=50,  # Updated number of epochs\n",
    "    per_device_train_batch_size=512,  # Updated batch size\n",
    "    per_device_eval_batch_size=512,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,  # Load best model at the end\n",
    "    # fp16=True,  # Enable mixed precision training\n",
    "    gradient_accumulation_steps=2,  # Simulate larger batch size\n",
    "    learning_rate=1e-4  # Updated learning rate\n",
    ")\n",
    "\n",
    "# Trainer with EarlyStoppingCallback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_split,  # Updated to use split data\n",
    "    eval_dataset=val_dataset,  # Updated to use validation data\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]  # Updated early stopping\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model and print results\n",
    "results = trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "9kNHBFNCqGnQ",
    "outputId": "5fbda4be-e2aa-4db4-9e50-d66569a3d55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 1.1855469 0.5124512 0.19116211 1.0292969 0.5078125 0.35766602 0.9499512 0.7338867 0.80615234 1.1665039 0.8579102 1.0546875 1.2587892 0.7905274 1.1374512 1.0974121 0.3955078 1.1479492 0.28881836 0.20166017 1.142334 0.001220703 -0.5805664 1.279297 -0.56884766 -1.6184083 1.4101563 -0.5275879 -1.7941896 0.98657227 -0.72875977 -1.9460449 0.7573242 -0.7702637 -1.5083008 0.4157715 -0.5095215 -1.6040039 0.4375 -0.39453128 -1.4926758 0.23291016 -0.22192383 -1.0761719 0.26586914 -0.20288086 -1.3786621 0.047607422 -0.27270508 -0.9104004 -0.087890625 -0.2775879 -1.0041504 -0.07666016 -0.2932129 -0.8710938 0.011962892 -0.34057617 -0.7502442 0.13989258 -0.34057617 -0.7502442 0.13989258 -0.41137698 -0.56762695 0.23486328 -0.30639648 -0.35546875 0.42626953 -0.16040039 -0.24780275 0.7351074 -0.24316406 -0.33593753 0.7641602 -0.16113281 -0.072509766 0.80322266 -0.060058594 -0.34423828 0.7402344 -0.07128906 -0.3820801 0.8305664 -0.07763672 -0.49731445 0.9152832 0.061523438 -0.36230472 0.9487305 0.17260742 -0.23266602 1.0319824 0.15063477 -0.2800293 1.0900879 0.09814453 -0.31225586 1.0297852 0.18408205 -0.37670898 1.0227051 0.11010742 -0.30029297 1.0270996 0.17309572 -0.31079102 1.0412598 0.13916016 -0.33251953 0.8776856 0.17944336 -0.21972656 0.88208014 0.46069336 -0.3110352 0.95336914 0.46337894 -0.3886719 0.83691406\n",
      "Tokenized input_ids: tensor([[  101,  1015,  1012,  8492, 21472,  2683,  1014,  1012, 24406, 19961,\n",
      "         12521,  1014,  1012,  5184,  2575, 17465,  2487,  1015,  1012,  6185,\n",
      "          2683, 24594,  2575,  2683,  1014,  1012,  2753,  2581,  2620, 12521,\n",
      "          2629,  1014,  1012, 26231, 28756, 16086,  2475,  1014,  1012,   102]])\n",
      "Tokenized attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Example 1\n",
      "Text: 1.1855469 0.5124512 0.19116211 1.0292969 0.5078125 0.35766602 0.9499512 0.7338867 0.80615234 1.1665039 0.8579102 1.0546875 1.2587892 0.7905274 1.1374512 1.0974121 0.3955078 1.1479492 0.28881836 0.20166017 1.142334 0.001220703 -0.5805664 1.279297 -0.56884766 -1.6184083 1.4101563 -0.5275879 -1.7941896 0.98657227 -0.72875977 -1.9460449 0.7573242 -0.7702637 -1.5083008 0.4157715 -0.5095215 -1.6040039 0.4375 -0.39453128 -1.4926758 0.23291016 -0.22192383 -1.0761719 0.26586914 -0.20288086 -1.3786621 0.047607422 -0.27270508 -0.9104004 -0.087890625 -0.2775879 -1.0041504 -0.07666016 -0.2932129 -0.8710938 0.011962892 -0.34057617 -0.7502442 0.13989258 -0.34057617 -0.7502442 0.13989258 -0.41137698 -0.56762695 0.23486328 -0.30639648 -0.35546875 0.42626953 -0.16040039 -0.24780275 0.7351074 -0.24316406 -0.33593753 0.7641602 -0.16113281 -0.072509766 0.80322266 -0.060058594 -0.34423828 0.7402344 -0.07128906 -0.3820801 0.8305664 -0.07763672 -0.49731445 0.9152832 0.061523438 -0.36230472 0.9487305 0.17260742 -0.23266602 1.0319824 0.15063477 -0.2800293 1.0900879 0.09814453 -0.31225586 1.0297852 0.18408205 -0.37670898 1.0227051 0.11010742 -0.30029297 1.0270996 0.17309572 -0.31079102 1.0412598 0.13916016 -0.33251953 0.8776856 0.17944336 -0.21972656 0.88208014 0.46069336 -0.3110352 0.95336914 0.46337894 -0.3886719 0.83691406\n",
      "Label: 0\n",
      "Tokenized input_ids: [101, 1015, 1012, 8492, 21472, 2683, 1014, 1012, 24406, 19961, 12521, 1014, 1012, 5184, 2575, 17465, 2487, 1015, 1012, 6185, 2683, 24594, 2575, 2683, 1014, 1012, 2753, 2581, 2620, 12521, 2629, 1014, 1012, 26231, 28756, 16086, 2475, 1014, 1012, 102]\n",
      "Tokenized attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Text: 0.5812988 0.39379886 0.29052734 0.49951172 0.40966797 0.28588867 0.42700195 0.4272461 0.2788086 0.5415039 0.5402832 0.4482422 0.5415039 0.5402832 0.4482422 0.6262207 0.6274414 0.5625 0.6228028 0.61938477 0.6911621 0.32421878 0.5109863 0.7680664 0.29418945 0.4355469 0.7529297 0.06665039 0.28857422 0.5546875 0.28344727 -0.45410156 0.26416016 0.28344727 -0.45410156 0.26416016 0.5678711 -0.59765625 -0.100830086 0.7155762 -1.2036133 -0.6066895 1.5107423 -0.92895514 -2.145996 5.694824 7.9990234 -0.3010254 2.7395022 2.0078125 0.33203125 -1.5163574 0.7285156 -0.7470703 0.33203125 1.3190918 -0.072509766 -0.4619141 1.2255859 -0.37988284 -0.71484375 0.5300293 0.028320314 -0.46435547 0.87158203 -0.72924805 -0.15307617 0.64184576 -0.96313477 -0.31323245 0.53149414 -0.86987305 -0.21362305 0.6020508 -0.8942871 -0.23046875 0.5148926 -0.48828128 -0.25634766 0.47656253 -0.88647467 -0.6086426 0.42382815 -0.7602539 -0.39282227 0.33544922 -0.99438477 -0.57836914 0.22656251 -0.842041 -0.484375 0.18334962 -0.83618164 -0.484375 0.18334962 -0.83618164 -0.48657227 0.12988281 -0.8071289 -0.486084 0.12841797 -0.78247076 -0.472168 0.20092775 -0.888916 -0.51342773 0.23242189 -0.8701172 -0.4956055 0.29296875 -0.8876953 -0.5019531 0.30615234 -0.88012695 -0.5197754 0.33886722 -0.8078613 -0.5024414 0.40429688 -0.8535157\n",
      "Label: 1\n",
      "Tokenized input_ids: [101, 1014, 1012, 5388, 12521, 2683, 2620, 2620, 1014, 1012, 4464, 24434, 2683, 2620, 20842, 1014, 1012, 17222, 25746, 2581, 22022, 1014, 1012, 4749, 2683, 22203, 16576, 2475, 1014, 1012, 2871, 2683, 28756, 2581, 2683, 2581, 1014, 1012, 21777, 102]\n",
      "Tokenized attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1165, 2912]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Calculate evaluation metrics\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_val, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1165, 2912]"
     ]
    }
   ],
   "source": [
    "# Inspect the tokenization process\n",
    "sample_text = X_train_text[0]\n",
    "encoding = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Tokenized input_ids:\", encoding['input_ids'])\n",
    "print(\"Tokenized attention_mask:\", encoding['attention_mask'])\n",
    "\n",
    "# Inspect a few examples from the dataset\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i+1}\")\n",
    "    print(\"Text:\", X_train_text[i])\n",
    "    print(\"Label:\", y_train[i])\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        X_train_text[i],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    print(\"Tokenized input_ids:\", encoding['input_ids'].flatten().tolist())\n",
    "    print(\"Tokenized attention_mask:\", encoding['attention_mask'].flatten().tolist())\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = trainer.predict(test_dataset)  # Predict on test dataset\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Ensure y_val and y_pred are numpy arrays\n",
    "y_val = np.array(y_val)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "precision = precision_score(y_val, y_pred, average='weighted')\n",
    "recall = recall_score(y_val, y_pred, average='weighted')\n",
    "f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(y_val, predictions.predictions[:, 1], average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_val, y_pred, target_names=['Not Fall', 'Fall'], digits=4)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "KcVIQEqHX9ow",
    "outputId": "f47fe55b-0903-4c73-ba8c-3cb40812d652"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for the validation set\n",
    "fpr, tpr, _ = roc_curve(y_val, predictions.predictions[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jmLTOfXoYBVY",
    "outputId": "3110a6c8-2789-40ea-a4aa-fc8f2cd14d95"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred, labels=[0, 1])  # Assuming 0 is 'Not Fall' and 1 is 'Fall'\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fall', 'Fall'], yticklabels=['Not Fall', 'Fall'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
